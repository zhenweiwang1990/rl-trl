model_name: "unsloth/Qwen3-32B"
max_seq_length: 2048
load_in_4bit: true
lora_r: 8
lora_alpha: 8
lora_dropout: 0.0
target_modules: [q_proj, k_proj, v_proj, o_proj]
dataset_name: "openai/gsm8k"
output_dir: "outputs/test"
num_train_epochs: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 5.0e-5
warmup_steps: 10
logging_steps: 5
save_steps: 50
eval_steps: 50
save_total_limit: 2
num_generations: 2
max_prompt_length: 512
max_completion_length: 256
temperature: 0.7
top_p: 0.9
beta: 0.01
seed: 42
bf16: true
gradient_checkpointing: true
optim: "adamw_8bit"
max_grad_norm: 1.0
report_to: "none"
wandb_project: "test"