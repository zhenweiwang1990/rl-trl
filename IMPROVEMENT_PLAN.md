# RL-TRL è®­ç»ƒæµç¨‹æ”¹è¿›è®¡åˆ’

åŸºäº rl-unsloth é¡¹ç›®çš„æœ€ä½³å®è·µï¼Œæ”¹è¿› rl-trl çš„è®­ç»ƒè¯„ä¼°ã€ä¿å­˜ã€æ–­ç‚¹ç»­è®­ç­‰åŠŸèƒ½ã€‚

## ğŸ“‹ æ”¹è¿›æ€»è§ˆ

### 1ï¸âƒ£ ä¼˜å…ˆçº§ P0 - æ ¸å¿ƒåŠŸèƒ½å¢å¼º

#### 1.1 Checkpoint ä¿å­˜å¢å¼º
**å½“å‰çŠ¶æ€**:
- âœ… å·²ä¿å­˜: model, tokenizer, training_state.json
- âŒ ç¼ºå¤±: optimizer çŠ¶æ€, training_metadata.json, evals_without_improvement

**æ”¹è¿›å†…å®¹**:
```python
# _save_checkpoint() å¢å¼º
1. ä¿å­˜ optimizer çŠ¶æ€åˆ° optimizer.pt
2. åˆ›å»ºç‹¬ç«‹çš„ training_metadata.json æ–‡ä»¶ï¼ˆåŒ…å«è¯¦ç»†çš„ metricsï¼‰
3. åœ¨ training_state.json ä¸­æ·»åŠ  evals_without_improvement å­—æ®µ
4. ä¿å­˜æ›´å¤š metrics åˆ° metadataï¼ˆåŒ…æ‹¬ avg_score, avg_hits ç­‰ï¼‰
```

**å—ç›Š**:
- å®Œæ•´æ¢å¤è®­ç»ƒçŠ¶æ€ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨åŠ¨é‡
- æ›´å‡†ç¡®çš„æ–­ç‚¹ç»­è®­
- æ›´è¯¦ç»†çš„ checkpoint å…ƒæ•°æ®ä¾›åç»­åˆ†æ

#### 1.2 Checkpoint åŠ è½½å¢å¼º
**å½“å‰çŠ¶æ€**:
- âœ… å·²åŠ è½½: global_step, best_accuracy, best_model_path
- âŒ ç¼ºå¤±: optimizer çŠ¶æ€, evals_without_improvement

**æ”¹è¿›å†…å®¹**:
```python
# _load_checkpoint() å¢å¼º
1. åŠ è½½ optimizer çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
2. åŠ è½½ evals_without_improvementï¼ˆç”¨äº early stoppingï¼‰
3. æ‰“å°æ›´è¯¦ç»†çš„æ¢å¤ä¿¡æ¯
```

#### 1.3 è¯„ä¼°æ—¥å¿—ä¿å­˜
**å½“å‰çŠ¶æ€**:
- âŒ è¯„ä¼°ç»“æœåªæ‰“å°åˆ°æ§åˆ¶å°ï¼Œæœªä¿å­˜åˆ°æ–‡ä»¶
- âŒ ç¼ºå°‘è¯¦ç»†çš„ rubric ç»Ÿè®¡

**æ”¹è¿›å†…å®¹**:
```python
# _run_evaluation() å¢å¼º
1. åˆ›å»º eval_logs/ ç›®å½•
2. ä¿å­˜æ¯æ¬¡è¯„ä¼°ç»“æœåˆ° eval_step_XXXX.json
3. æ·»åŠ è¯¦ç»†ç»Ÿè®¡:
   - åŸºæœ¬ç»Ÿè®¡: step, accuracy, correct_answers, total_samples
   - å¥–åŠ±ç»Ÿè®¡: avg_reward, median_reward, std_reward, min/max_reward
   - è¯¦ç»†æŒ‡æ ‡: 
     * attempted_answer (å°è¯•å›ç­”çš„æ•°é‡)
     * found_correct_profile (æ‰¾åˆ°æ­£ç¡® profile çš„æ•°é‡)
     * read_correct_profile (è¯»å–æ­£ç¡® profile çš„æ•°é‡)
     * avg_turns (å¹³å‡è½®æ•°)
     * avg_search_attempts (å¹³å‡æœç´¢æ¬¡æ•°)
   - è¯„ä¼°æ—¶é—´: eval_time
```

**JSON æ ¼å¼ç¤ºä¾‹**:
```json
{
  "step": 10,
  "accuracy": 0.75,
  "correct_answers": 75,
  "total_samples": 100,
  "attempted_answer": 95,
  "avg_reward": 0.623,
  "median_reward": 0.750,
  "std_reward": 0.412,
  "min_reward": -1.0,
  "max_reward": 1.5,
  "found_correct_profile": 80,
  "read_correct_profile": 78,
  "avg_turns": 4.2,
  "avg_search_attempts": 2.8,
  "eval_time": 234.5
}
```

#### 1.4 Baseline è¯„ä¼°å¢å¼º
**å½“å‰çŠ¶æ€**:
- âœ… å·²æœ‰ run_baseline_eval æ ‡å¿—å’ŒåŸºæœ¬å®ç°
- âŒ Baseline ç»“æœæœªä¿å­˜
- âŒ æ— æ³•è·³è¿‡ baselineï¼ˆé‡å¤è¿è¡Œæµªè´¹æ—¶é—´ï¼‰

**æ”¹è¿›å†…å®¹**:
```python
# train() æ–¹æ³•ä¸­çš„ baseline è¯„ä¼°å¢å¼º
1. ä¿å­˜ baseline ç»“æœåˆ° baseline_eval.json
2. æ·»åŠ æ—¶é—´æˆ³å’Œå®Œæ•´ç»Ÿè®¡
3. å¦‚æœ RUN_BASELINE_EVAL=false ä¸” baseline_eval.json å­˜åœ¨ï¼Œåˆ™ä»æ–‡ä»¶åŠ è½½
4. æ‰“å°æ›´æ¸…æ™°çš„ baseline ç»“æœ
```

---

### 2ï¸âƒ£ ä¼˜å…ˆçº§ P1 - æ—¥å¿—å’Œç›‘æ§å¢å¼º

#### 2.1 è®­ç»ƒæ­¥éª¤è¯¦ç»†æ—¥å¿—
**å½“å‰çŠ¶æ€**:
- âœ… åŸºæœ¬çš„æ­¥éª¤æ—¥å¿—
- âŒ ç¼ºå°‘é˜¶æ®µåˆ’åˆ†å’Œè¯¦ç»†ç»Ÿè®¡

**æ”¹è¿›å†…å®¹**:
```python
# train() ä¸»å¾ªç¯å¢å¼º
è¯¦ç»†æ¨¡å¼ï¼ˆVERBOSE=trueï¼‰ä¸‹æ˜¾ç¤º:
1. STEP å¼€å§‹æ ‡è®°
2. Rollout é˜¶æ®µæ—¥å¿—
3. Advantage è®¡ç®—é˜¶æ®µæ—¥å¿—
4. Backpropagation é˜¶æ®µæ—¥å¿—
5. Group ç»Ÿè®¡æ€»ç»“:
   - Total groups: X
   - Groups kept for training: Y
   - Groups filtered (low variance): Z
   - Rollouts finished early: N/M
   - Total rollout time: Xs
   - Total training time: Ys
   - Trainable tokens: X/Y (Z%)
```

#### 2.2 è¯„ä¼°ç»“æœå¢å¼ºæ‰“å°
**å½“å‰çŠ¶æ€**:
- âœ… å·²æœ‰åŸºæœ¬çš„è¯„ä¼°æ—¥å¿—æ‰“å°
- âœ… å·²å®ç°è¯¦ç»†çš„å¼€å§‹å’Œç»“æŸæ—¥å¿—

**æ”¹è¿›å†…å®¹**:
```python
# _run_evaluation() æ‰“å°å¢å¼º
1. æ·»åŠ ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€æœ€å°/æœ€å¤§å¥–åŠ±
2. æ·»åŠ è¯¦ç»† rubric ç»Ÿè®¡
3. æ·»åŠ è¯„ä¼°è€—æ—¶
4. ä¿å­˜ä½ç½®æç¤º
```

#### 2.3 Wandb æŒ‡æ ‡è¡¥å……
**å½“å‰çŠ¶æ€**:
- âœ… åŸºæœ¬çš„ train/eval æŒ‡æ ‡
- âŒ ç¼ºå°‘è¯¦ç»†çš„ rubric æŒ‡æ ‡

**æ”¹è¿›å†…å®¹**:
```python
# Wandb æ—¥å¿—å¢å¼º
è®­ç»ƒé˜¶æ®µæ–°å¢:
- train/median_reward
- train/groups_kept
- train/groups_filtered
- train/num_early_exit
- train/trainable_token_ratio

è¯„ä¼°é˜¶æ®µæ–°å¢:
- eval/median_reward
- eval/std_reward
- eval/attempted_answer_rate
- eval/found_profile_rate
- eval/read_profile_rate
- eval/avg_turns
- eval/avg_search_attempts
```

---

### 3ï¸âƒ£ ä¼˜å…ˆçº§ P2 - é«˜çº§åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰

#### 3.1 TrainingMetrics å­—æ®µè¡¥å……
**æ”¹è¿›å†…å®¹**:
```python
# åœ¨ TrainingMetrics ä¸­æ·»åŠ ï¼ˆå·²åœ¨ utils.py å®šä¹‰ä½†æœªå……åˆ†ä½¿ç”¨ï¼‰:
- avg_score: float (å½“å‰çš„ rubric score)
- avg_hits: float (å¹³å‡æ‰¾åˆ°çš„æ­£ç¡® handle æ•°)
```

#### 3.2 Training Summaryï¼ˆè®­ç»ƒç»“æŸæ€»ç»“ï¼‰
**æ”¹è¿›å†…å®¹**:
```python
# train() æ–¹æ³•ç»“æŸæ—¶
1. æ‰“å°è®­ç»ƒæ€»ç»“:
   - æ€»è®­ç»ƒæ­¥æ•°
   - æœ€ä½³å‡†ç¡®ç‡
   - æœ€ä½³æ¨¡å‹è·¯å¾„
   - æ€»è®­ç»ƒæ—¶é—´
   - æ˜¯å¦è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡
2. å¦‚æœæœ‰ early stoppingï¼Œè¯´æ˜åŸå› 
```

---

## ğŸ”§ å…·ä½“å®ç°æ­¥éª¤

### Step 1: å¢å¼º TrainingMetricsï¼ˆrl-trl/link_search_agent/trainer.pyï¼‰
```python
# åœ¨ TrainingMetrics dataclass ä¸­æ·»åŠ ç¼ºå¤±å­—æ®µ
@dataclass
class TrainingMetrics:
    # ... ç°æœ‰å­—æ®µ ...
    avg_score: float = 0.0  # å·²æœ‰
    avg_hits: float = 0.0   # å·²æœ‰
    median_reward: float = 0.0  # æ–°å¢
    # å…¶ä»–å­—æ®µå·²ç»åœ¨ grpo/utils.py ä¸­å®šä¹‰
```

### Step 2: å¢å¼º _save_checkpoint
```python
def _save_checkpoint(self, metrics: TrainingMetrics):
    """Save model checkpoint and training state."""
    checkpoint_dir = self.output_dir / f"checkpoint-{self.global_step}"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. Save model
    self.model.save_pretrained(str(checkpoint_dir))
    self.tokenizer.save_pretrained(str(checkpoint_dir))
    
    # 2. Save optimizer state (NEW)
    torch.save(self.optimizer.state_dict(), checkpoint_dir / "optimizer.pt")
    
    # 3. Save training state (ENHANCED)
    training_state = {
        "global_step": self.global_step,
        "best_accuracy": self.best_accuracy,
        "best_model_path": str(self.best_model_path) if self.best_model_path else None,
        "evals_without_improvement": self.evals_without_improvement,  # NEW
    }
    with open(checkpoint_dir / "training_state.json", 'w') as f:
        json.dump(training_state, f, indent=2)
    
    # 4. Save training metadata (NEW)
    training_metadata = {
        "step": self.global_step,
        "accuracy": metrics.accuracy,
        "metrics": {
            "loss": metrics.loss,
            "policy_loss": metrics.policy_loss,
            "kl_loss": metrics.kl_loss,
            "avg_reward": metrics.avg_reward,
            "median_reward": metrics.median_reward,
            "avg_score": metrics.avg_score,
            "avg_hits": metrics.avg_hits,
            "reward_std": metrics.reward_std,
        }
    }
    with open(checkpoint_dir / "training_metadata.json", 'w') as f:
        json.dump(training_metadata, f, indent=2)
    
    logger.info(f"ğŸ’¾ Model and training state saved to: {checkpoint_dir}")
    return checkpoint_dir
```

### Step 3: å¢å¼º _load_checkpoint
```python
def _load_checkpoint(self, checkpoint_path: Path):
    """Load training state from checkpoint."""
    logger.info(f"ğŸ“‚ Loading training state from checkpoint: {checkpoint_path}")
    
    # 1. Load optimizer state (NEW)
    optimizer_path = checkpoint_path / "optimizer.pt"
    if optimizer_path.exists():
        self.optimizer.load_state_dict(torch.load(optimizer_path))
        logger.info("âœ“ Optimizer state loaded")
    else:
        logger.warning("âš ï¸  Optimizer state not found, starting with fresh optimizer")
    
    # 2. Load training state (ENHANCED)
    state_file = checkpoint_path / "training_state.json"
    if state_file.exists():
        with open(state_file, 'r') as f:
            state = json.load(f)
        
        self.global_step = state.get("global_step", 0)
        self.best_accuracy = state.get("best_accuracy", 0.0)
        best_model_path_str = state.get("best_model_path")
        self.best_model_path = Path(best_model_path_str) if best_model_path_str else None
        self.evals_without_improvement = state.get("evals_without_improvement", 0)  # NEW
        
        logger.info(f"âœ“ Training state loaded:")
        logger.info(f"  - Global step: {self.global_step}")
        logger.info(f"  - Best accuracy: {self.best_accuracy:.2%}")
        logger.info(f"  - Evals without improvement: {self.evals_without_improvement}")
    else:
        logger.warning("âš ï¸  Training state not found, starting from step 0")
```

### Step 4: å¢å¼º _run_evaluationï¼ˆä¿å­˜åˆ°æ–‡ä»¶ï¼‰
```python
def _run_evaluation(self, is_baseline: bool = False) -> TrainingMetrics:
    """Run evaluation on eval set."""
    # ... ç°æœ‰çš„è¯„ä¼°ä»£ç  ...
    
    # Collect detailed statistics (NEW)
    rubrics = [s.rubric for g in groups for s in g.samples]
    
    eval_stats = {
        "step": self.global_step if not is_baseline else -1,
        "is_baseline": is_baseline,
        "accuracy": accuracy,
        "correct_answers": int(accuracy * len(rubrics)),
        "total_samples": len(rubrics),
        "avg_reward": avg_reward,
        "median_reward": float(np.median(rewards)) if rewards else 0.0,
        "std_reward": float(np.std(rewards)) if rewards else 0.0,
        "min_reward": float(min(rewards)) if rewards else 0.0,
        "max_reward": float(max(rewards)) if rewards else 0.0,
        "avg_score": avg_score,
        "avg_hits": avg_hits,
        "eval_time": eval_time,
    }
    
    # Save to file (NEW)
    if is_baseline:
        eval_log_file = self.output_dir / "baseline_eval.json"
    else:
        eval_log_dir = self.output_dir / "eval_logs"
        eval_log_dir.mkdir(parents=True, exist_ok=True)
        eval_log_file = eval_log_dir / f"eval_step_{self.global_step:04d}.json"
    
    with open(eval_log_file, "w") as f:
        json.dump(eval_stats, f, indent=2)
    
    logger.info(f"ğŸ’¾ Eval stats saved to: {eval_log_file}")
    
    # ... è¿”å› metrics ...
```

### Step 5: å¢å¼ºè®­ç»ƒå¾ªç¯æ—¥å¿—
```python
def train(self):
    """Main training loop."""
    # ... baseline eval ...
    
    for step in range(self.global_step, self.max_steps):
        # ... 
        
        # Collect metrics with more detail
        all_rewards = [s.reward for g in groups for s in g.samples]
        all_scores = [s.rubric.score for g in groups for s in g.samples]
        
        # Calculate additional metrics (NEW)
        median_reward = float(np.median(all_rewards)) if all_rewards else 0.0
        num_early_exit = sum(
            1 for g in groups for s in g.samples 
            if len(s.conversation) < self.policy_config.max_turns * 2
        )
        
        metrics = TrainingMetrics(
            # ... ç°æœ‰å­—æ®µ ...
            median_reward=median_reward,  # NEW
            groups_kept=len([g for g in groups if any(s.advantage and s.advantage != 0 for s in g.samples)]),
            groups_filtered=len([g for g in groups if all(s.advantage is None or s.advantage == 0 for s in g.samples)]),
            num_early_exit=num_early_exit,  # NEW
        )
        
        # Print detailed group summary (NEW, if VERBOSE)
        if self.verbose:
            print(f"\nğŸ“Š Group Summary:", flush=True)
            print(f"  - Total groups: {len(groups)}", flush=True)
            print(f"  - Groups kept for training: {metrics.groups_kept}", flush=True)
            print(f"  - Groups filtered (low variance): {metrics.groups_filtered}", flush=True)
            print(f"  - Rollouts finished early: {num_early_exit}/{len(all_rewards)}", flush=True)
            print(f"  - Total rollout time: {rollout_time:.1f}s", flush=True)
            print(f"  - Total training time: {train_time:.1f}s", flush=True)
        
        # Enhanced wandb logging (NEW)
        if self.use_wandb:
            wandb.log({
                # ... ç°æœ‰æŒ‡æ ‡ ...
                "train/median_reward": median_reward,
                "train/groups_kept": metrics.groups_kept,
                "train/groups_filtered": metrics.groups_filtered,
                "train/num_early_exit": num_early_exit,
            }, step=self.global_step)
```

---

## âœ… éªŒæ”¶æ ‡å‡†

å®Œæˆååº”è¯¥å…·å¤‡ä»¥ä¸‹èƒ½åŠ›:

1. **Checkpoint å®Œæ•´æ€§**
   - âœ… checkpoint ç›®å½•åŒ…å«: adapter_model.*, optimizer.pt, training_state.json, training_metadata.json
   - âœ… æ–­ç‚¹ç»­è®­èƒ½æ¢å¤ optimizer çŠ¶æ€å’Œ early stopping è®¡æ•°

2. **è¯„ä¼°æ—¥å¿—**
   - âœ… æ¯æ¬¡è¯„ä¼°ç”Ÿæˆ eval_step_XXXX.json
   - âœ… Baseline è¯„ä¼°ç”Ÿæˆ baseline_eval.json
   - âœ… JSON åŒ…å«å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯

3. **è®­ç»ƒæ—¥å¿—**
   - âœ… è¯¦ç»†æ¨¡å¼ä¸‹æ˜¾ç¤º group ç»Ÿè®¡
   - âœ… æ˜¾ç¤º early exit rollout æ•°é‡
   - âœ… æ˜¾ç¤º trainable token æ¯”ä¾‹

4. **Wandb ç›‘æ§**
   - âœ… åŒ…å« median_reward, groups_kept/filtered, early_exit ç­‰æŒ‡æ ‡
   - âœ… è¯„ä¼°æŒ‡æ ‡åŒ…å«è¯¦ç»†çš„ rubric ç»Ÿè®¡

5. **ç”¨æˆ·ä½“éªŒ**
   - âœ… RUN_BASELINE_EVAL=false æ—¶è‡ªåŠ¨åŠ è½½å·²æœ‰ baseline
   - âœ… æ–­ç‚¹ç»­è®­æ‰“å°è¯¦ç»†çš„æ¢å¤ä¿¡æ¯
   - âœ… è®­ç»ƒç»“æŸæ‰“å°æ€»ç»“ä¿¡æ¯

---

## ğŸ“… å®æ–½æ—¶é—´ä¼°è®¡

- **P0 æ ¸å¿ƒåŠŸèƒ½**: 2-3 å°æ—¶
  - Checkpoint å¢å¼º: 1 å°æ—¶
  - è¯„ä¼°æ—¥å¿—ä¿å­˜: 1 å°æ—¶
  - Baseline å¢å¼º: 0.5 å°æ—¶

- **P1 æ—¥å¿—ç›‘æ§**: 1-2 å°æ—¶
  - è®­ç»ƒæ—¥å¿—å¢å¼º: 1 å°æ—¶
  - Wandb æŒ‡æ ‡è¡¥å……: 0.5 å°æ—¶

- **P2 é«˜çº§åŠŸèƒ½**: 0.5-1 å°æ—¶
  - Training summary: 0.5 å°æ—¶

**æ€»è®¡**: 3.5-6 å°æ—¶

---

## ğŸ” æµ‹è¯•è®¡åˆ’

1. **åŠŸèƒ½æµ‹è¯•**
   - ä»å¤´å¼€å§‹è®­ç»ƒ -> æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
   - ä¸­æ–­è®­ç»ƒ -> æ–­ç‚¹ç»­è®­ -> éªŒè¯çŠ¶æ€æ¢å¤
   - è¿è¡Œ baseline -> å…³é—­ baseline -> éªŒè¯ä»æ–‡ä»¶åŠ è½½

2. **å…¼å®¹æ€§æµ‹è¯•**
   - æ—§çš„ checkpoint èƒ½å¦åŠ è½½ï¼ˆå‘åå…¼å®¹ï¼‰
   - ç¼ºå°‘ optimizer.pt æ—¶çš„é™çº§è¡Œä¸º

3. **æ€§èƒ½æµ‹è¯•**
   - è¯„ä¼°æ—¥å¿—ä¿å­˜ä¸åº”æ˜¾è‘—å½±å“æ€§èƒ½
   - æ–‡ä»¶å¤§å°åˆç†ï¼ˆJSON ä¸åº”è¿‡å¤§ï¼‰

---

## ğŸ“ æ–‡æ¡£æ›´æ–°

å®Œæˆåéœ€è¦æ›´æ–°:
1. QUICKSTART_LINKSEARCH.md - æ·»åŠ æ–°åŠŸèƒ½ä½¿ç”¨è¯´æ˜
2. åˆ›å»º TRAINING_LOGS.md - è¯¦ç»†è¯´æ˜æ—¥å¿—æ ¼å¼å’Œä½¿ç”¨æ–¹æ³•
3. env.linksearch.example - æ·»åŠ æ³¨é‡Šè¯´æ˜ RUN_BASELINE_EVAL

---

## ğŸ¯ é¢„æœŸæ•ˆæœ

å®æ–½åå°†å®ç°:
1. âœ… å®Œæ•´çš„è®­ç»ƒçŠ¶æ€ä¿å­˜å’Œæ¢å¤
2. âœ… å¯è¿½æº¯çš„è¯„ä¼°å†å²è®°å½•
3. âœ… æ›´æ¸…æ™°çš„è®­ç»ƒè¿›åº¦ç›‘æ§
4. âœ… æ›´å¥½çš„è°ƒè¯•å’Œé—®é¢˜å®šä½èƒ½åŠ›
5. âœ… ä¸ rl-unsloth é¡¹ç›®çš„åŠŸèƒ½å¯¹é½

