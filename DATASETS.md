# å¯ç”¨æ•°æ®é›†è¯´æ˜

æœ¬é¡¹ç›®æ”¯æŒå¤šä¸ªå…¬å¼€æ•°æ®é›†ç”¨äº GRPO è®­ç»ƒã€‚ä»¥ä¸‹æ˜¯æ¨èçš„æ•°æ®é›†åˆ—è¡¨ï¼š

## ğŸ§® æ•°å­¦æ¨ç†æ•°æ®é›†

### GSM8K (æ¨è)
- **æ•°æ®é›†åç§°**: `openai/gsm8k`
- **é…ç½®**: `main`
- **ä»»åŠ¡**: å°å­¦æ•°å­¦åº”ç”¨é¢˜
- **æ ·æœ¬æ•°**: 8.5K è®­ç»ƒæ ·æœ¬
- **éš¾åº¦**: éœ€è¦ 2-8 æ­¥æ¨ç†
- **é€‚ç”¨**: æ•°å­¦æ¨ç†ã€ç®—æœ¯è®¡ç®—

**ç¤ºä¾‹**:
```yaml
dataset_name: "openai/gsm8k"
```

**æ•°æ®æ ¼å¼**:
```json
{
  "question": "Natalia sold clips to 48 of her friends...",
  "answer": "Natalia sold 48/2 = <<48/2=24>>24 clips... #### 24"
}
```

## ğŸ“ æ–‡æœ¬æ‘˜è¦æ•°æ®é›†

### TLDR (Reddit æ‘˜è¦)
- **æ•°æ®é›†åç§°**: `trl-internal-testing/tldr-preference-trl-style`
- **ä»»åŠ¡**: Reddit å¸–å­æ‘˜è¦
- **æ ·æœ¬æ•°**: ~120K è®­ç»ƒæ ·æœ¬
- **æ ¼å¼**: prompt, chosen, rejected
- **é€‚ç”¨**: æ–‡æœ¬æ‘˜è¦ã€åå¥½å­¦ä¹ 

**ç¤ºä¾‹**:
```yaml
dataset_name: "trl-internal-testing/tldr-preference-trl-style"
```

### OpenAI Summarization Feedback
- **æ•°æ®é›†åç§°**: `openai/summarize_from_feedback`
- **é…ç½®**: `comparisons`
- **ä»»åŠ¡**: å¸¦äººç±»åé¦ˆçš„æ‘˜è¦
- **é€‚ç”¨**: RLHFã€åå¥½ä¼˜åŒ–

**ç¤ºä¾‹**:
```yaml
dataset_name: "openai/summarize_from_feedback"
```

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### 1. ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `configs/default.yaml`:

```yaml
# ä½¿ç”¨ GSM8K æ•°å­¦æ•°æ®é›†
dataset_name: "openai/gsm8k"

# æˆ–ä½¿ç”¨ TLDR æ‘˜è¦æ•°æ®é›†
# dataset_name: "trl-internal-testing/tldr-preference-trl-style"
```

### 2. è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

æ ¹æ®ä¸åŒæ•°æ®é›†ï¼Œéœ€è¦è°ƒæ•´ `train_grpo.py` ä¸­çš„ `reward_function`ï¼š

#### GSM8K å¥–åŠ±å‡½æ•° (é»˜è®¤)
```python
def reward_function(samples, prompts, outputs, **kwargs):
    rewards = []
    for prompt, output in zip(prompts, outputs):
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç­”æ¡ˆæ ‡è®° ####
        if "####" in output:
            reward = 1.0
        elif any(char.isdigit() for char in output):
            reward = 0.5  # éƒ¨åˆ†åˆ†æ•°
        else:
            reward = 0.0
        rewards.append(reward)
    return rewards
```

#### TLDR æ‘˜è¦å¥–åŠ±å‡½æ•°
```python
def reward_function(samples, prompts, outputs, **kwargs):
    rewards = []
    for prompt, output in zip(prompts, outputs):
        # åŸºäºæ‘˜è¦é•¿åº¦å’Œè´¨é‡è¯„åˆ†
        length = len(output.split())
        if 10 <= length <= 50:  # ç†æƒ³é•¿åº¦
            reward = 1.0
        elif length < 10:  # å¤ªçŸ­
            reward = 0.3
        else:  # å¤ªé•¿
            reward = 0.6
        rewards.append(reward)
    return rewards
```

## ğŸ”§ ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

### æ•°æ®æ ¼å¼è¦æ±‚

GRPO è®­ç»ƒéœ€è¦ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ï¼š

#### æ ¼å¼ 1: Question-Answer (GSM8K é£æ ¼)
```json
{
  "question": "é—®é¢˜æ–‡æœ¬",
  "answer": "ç­”æ¡ˆæ–‡æœ¬"
}
```

#### æ ¼å¼ 2: Prompt-Response
```json
{
  "prompt": "æç¤ºæ–‡æœ¬",
  "response": "å“åº”æ–‡æœ¬"
}
```

#### æ ¼å¼ 3: Preference (RLHF é£æ ¼)
```json
{
  "prompt": "æç¤ºæ–‡æœ¬",
  "chosen": "æ›´å¥½çš„å“åº”",
  "rejected": "è¾ƒå·®çš„å“åº”"
}
```

### ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®é›†

1. å‡†å¤‡æ•°æ®é›†ï¼ˆJSONL æˆ– CSV æ ¼å¼ï¼‰
2. ä¸Šä¼ åˆ° HuggingFace Hubï¼š

```python
from datasets import Dataset, load_dataset

# ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
dataset = load_dataset("json", data_files="my_data.jsonl")

# ä¸Šä¼ åˆ° Hub
dataset.push_to_hub("your-username/your-dataset-name")
```

3. åœ¨é…ç½®ä¸­ä½¿ç”¨ï¼š

```yaml
dataset_name: "your-username/your-dataset-name"
```

## ğŸ“Š æ•°æ®é›†å¯¹æ¯”

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | æ ·æœ¬æ•° | éš¾åº¦ | æ¨èç”¨é€” |
|--------|---------|-------|------|---------|
| GSM8K | æ•°å­¦æ¨ç† | 8.5K | ä¸­ç­‰ | æ¨ç†èƒ½åŠ›è®­ç»ƒ |
| TLDR | æ–‡æœ¬æ‘˜è¦ | 120K | ç®€å• | æ‘˜è¦ç”Ÿæˆ |
| Summarize Feedback | æ‘˜è¦+RLHF | ~90K | ä¸­ç­‰ | åå¥½å¯¹é½ |

## ğŸ’¡ æç¤º

1. **é¦–æ¬¡ä½¿ç”¨**ï¼šGSM8K æ˜¯æœ€ç®€å•çš„èµ·ç‚¹ï¼Œæ•°æ®é›†å°ä¸”ä»»åŠ¡æ˜ç¡®
2. **å¤§è§„æ¨¡è®­ç»ƒ**ï¼šTLDR æä¾›æ›´å¤šæ ·æœ¬ï¼Œé€‚åˆé•¿æ—¶é—´è®­ç»ƒ
3. **RLHF åœºæ™¯**ï¼šä½¿ç”¨å¸¦æœ‰äººç±»åé¦ˆçš„æ•°æ®é›†ï¼ˆå¦‚ Summarize Feedbackï¼‰
4. **è‡ªå®šä¹‰ä»»åŠ¡**ï¼šå‡†å¤‡è‡ªå·±çš„æ•°æ®é›†å¹¶å®ç°å¯¹åº”çš„å¥–åŠ±å‡½æ•°

## ğŸ”— ç›¸å…³é“¾æ¥

- [GSM8K æ•°æ®é›†](https://huggingface.co/datasets/openai/gsm8k)
- [TLDR æ•°æ®é›†](https://huggingface.co/datasets/trl-internal-testing/tldr-preference-trl-style)
- [Summarize Feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)
- [TRL æ–‡æ¡£](https://huggingface.co/docs/trl)

