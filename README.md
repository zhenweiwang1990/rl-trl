# Qwen3-32B GRPO Training with TRL + Unsloth

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ TRL (Transformer Reinforcement Learning) å’Œ Unsloth è®­ç»ƒ Qwen3-32B æ¨¡å‹çš„å®Œæ•´é¡¹ç›®ï¼Œæ”¯æŒ GRPO (Group Relative Policy Optimization) è®­ç»ƒæ–¹æ³•ã€‚

## ğŸŒŸ ç‰¹æ€§

- âœ… åŸºäº NVIDIA PyTorch 25.11 å®¹å™¨
- âœ… ä½¿ç”¨ Unsloth è¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼ˆèŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿï¼‰
- âœ… æ”¯æŒ TRL çš„ GRPO è®­ç»ƒæ–¹æ³•
- âœ… æ”¯æŒå¤šç§ä»»åŠ¡ï¼š
  - **æ•°å­¦æ¨ç†**ï¼šä½¿ç”¨ TRL å®˜æ–¹çš„ GSM8K GRPO æ•°æ®é›†
  - **Link Search Agent**ï¼šLinkedIn ä¸ªäººèµ„æ–™æœç´¢å’ŒåŒ¹é…
- âœ… æ”¯æŒ LoRA å¾®è°ƒ
- âœ… æ”¯æŒ 4-bit é‡åŒ–è®­ç»ƒ
- âœ… Wandb é›†æˆç”¨äºå®éªŒè·Ÿè¸ª
- âœ… å®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°å’Œäº¤äº’æµ‹è¯•è„šæœ¬

## ğŸ“‹ é¡¹ç›®ç»“æ„

```
rl-trl/
â”œâ”€â”€ Dockerfile              # Docker é•œåƒé…ç½®
â”œâ”€â”€ requirements.txt        # Python ä¾èµ–
â”œâ”€â”€ train_grpo.py          # GRPO è®­ç»ƒè„šæœ¬ï¼ˆæ•°å­¦æ¨ç†ï¼‰
â”œâ”€â”€ train_grpo_linksearch.py  # Link Search Agent è®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval_model.py          # æ¨¡å‹è¯„ä¼°è„šæœ¬
â”œâ”€â”€ interactive_test.py    # äº¤äº’å¼æµ‹è¯•è„šæœ¬
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ default.yaml       # é»˜è®¤é…ç½®ï¼ˆæ•°å­¦æ¨ç†ï¼‰
â”‚   â”œâ”€â”€ linksearch.yaml    # Link Search é…ç½®
â”‚   â””â”€â”€ custom.yaml        # è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹
â”œâ”€â”€ link_search_agent/     # Link Search Agent æ¨¡å—
â”‚   â”œâ”€â”€ agent.py          # Agent å®ç°
â”‚   â”œâ”€â”€ config.py         # é…ç½®
â”‚   â”œâ”€â”€ tools.py          # å·¥å…·å‡½æ•°ï¼ˆSQLæœç´¢ï¼‰
â”‚   â”œâ”€â”€ prompts.py        # æç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ rollout.py        # Rollout å’Œ reward è®¡ç®—
â”‚   â”œâ”€â”€ trainer.py        # è‡ªå®šä¹‰ GRPO Trainer
â”‚   â””â”€â”€ data/             # æ•°æ®åŠ è½½
â”œâ”€â”€ grpo/                 # GRPO é€šç”¨å·¥å…·
â”‚   â”œâ”€â”€ callbacks.py      # è®­ç»ƒå›è°ƒ
â”‚   â””â”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ scripts/               # è¾…åŠ©è„šæœ¬
â”‚   â”œâ”€â”€ build.sh          # æ„å»º Docker é•œåƒ
â”‚   â”œâ”€â”€ run.sh            # è¿è¡Œå®¹å™¨
â”‚   â””â”€â”€ train.sh          # å¯åŠ¨è®­ç»ƒ
â”œâ”€â”€ outputs/              # è®­ç»ƒè¾“å‡ºï¼ˆcheckpointsï¼‰
â”œâ”€â”€ logs/                 # è®­ç»ƒæ—¥å¿—
â””â”€â”€ data/                 # æ•°æ®ç¼“å­˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ä½¿ç”¨ Dockerï¼ˆæ¨èï¼‰

#### 1. æ„å»º Docker é•œåƒ

```bash
cd /home/zhlmmc/rl-trl
bash scripts/build.sh
```

#### 2. è¿è¡Œå®¹å™¨

```bash
bash scripts/run.sh
```

#### 3. åœ¨å®¹å™¨å†…å¼€å§‹è®­ç»ƒ

**æ•°å­¦æ¨ç†ä»»åŠ¡ï¼ˆGSM8Kï¼‰ï¼š**
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python train_grpo.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python train_grpo.py --config configs/custom.yaml

# ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
python train_grpo.py --model unsloth/Qwen3-32B --load_in_4bit
```

**Link Search Agent ä»»åŠ¡ï¼š**
```bash
# ä½¿ç”¨ masked æ¨¡å¼ï¼ˆæ¨èï¼‰
python train_grpo_linksearch.py --mode masked

# ä½¿ç”¨ rollout æ¨¡å¼
python train_grpo_linksearch.py --mode rollout

# ä½¿ç”¨ simple æ¨¡å¼ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
python train_grpo_linksearch.py --mode simple

# å¯ç”¨è¯¦ç»†æ—¥å¿—
python train_grpo_linksearch.py --mode masked --enable-detailed-logging
```

### æ–¹å¼ 2: æœ¬åœ°è¿è¡Œ

#### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

#### 2. å¼€å§‹è®­ç»ƒ

**æ•°å­¦æ¨ç†ï¼š**
```bash
python train_grpo.py
```

**Link Search Agentï¼š**
```bash
# 1. å‡†å¤‡æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
bash scripts/generate_database.sh  # ä» PostgreSQL ç”Ÿæˆ
# æˆ–
cp /path/to/profiles.db link_search_agent/data/profiles.db

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export PROFILE_DB_PATH="/path/to/profiles.db"
export HF_TOKEN="your_huggingface_token"

# 3. å¼€å§‹è®­ç»ƒ
python train_grpo_linksearch.py --mode masked
```

## ğŸ“Š è®­ç»ƒé…ç½®

ä¸»è¦é…ç½®å‚æ•°ï¼ˆåœ¨ `configs/default.yaml` ä¸­ï¼‰ï¼š

```yaml
# æ¨¡å‹è®¾ç½®
model_name: "unsloth/Qwen3-32B"
max_seq_length: 4096
load_in_4bit: true

# LoRA è®¾ç½®
lora_r: 16
lora_alpha: 16

# è®­ç»ƒè®¾ç½®
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 5.0e-5

# GRPO è®¾ç½®
num_generations: 4
max_prompt_length: 1024
max_completion_length: 512
temperature: 0.7
beta: 0.01
```

## ğŸ¯ ä½¿ç”¨ä¸åŒæ•°æ®é›†

é»˜è®¤ä½¿ç”¨ OpenAI çš„ GSM8K æ•°å­¦æ•°æ®é›†ï¼ˆ`openai/gsm8k`ï¼‰ã€‚

### å¯ç”¨æ•°æ®é›†ï¼š

1. **GSM8K** (é»˜è®¤) - æ•°å­¦æ¨ç†
   ```yaml
   dataset_name: "openai/gsm8k"
   ```

2. **TLDR** - æ–‡æœ¬æ‘˜è¦
   ```yaml
   dataset_name: "trl-internal-testing/tldr-preference-trl-style"
   ```

3. **Summarize Feedback** - RLHF æ‘˜è¦
   ```yaml
   dataset_name: "openai/summarize_from_feedback"
   ```

è¯¦è§ [DATASETS.md](DATASETS.md) è·å–å®Œæ•´çš„æ•°æ®é›†è¯´æ˜å’Œä½¿ç”¨æŒ‡å—ã€‚

è¦ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ï¼š

1. ä¿®æ”¹ `configs/custom.yaml` ä¸­çš„ `dataset_name`
2. æ ¹æ®éœ€è¦ä¿®æ”¹ `train_grpo.py` ä¸­çš„ `reward_function`

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### Wandb

é¡¹ç›®é»˜è®¤ä½¿ç”¨ Wandb è¿›è¡Œè®­ç»ƒç›‘æ§ï¼š

```bash
# ç™»å½• Wandb
wandb login

# è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨ä¸Šä¼ æŒ‡æ ‡
python train_grpo.py
```

è¦ç¦ç”¨ Wandbï¼š

```bash
python train_grpo.py --no_wandb
```

### TensorBoard

æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š

```bash
tensorboard --logdir outputs/qwen3-32b-grpo
```

## ğŸ§ª è¯„ä¼°å’Œæµ‹è¯•

### è¯„ä¼°æ¨¡å‹

```bash
python eval_model.py --checkpoint outputs/qwen3-32b-grpo/final
```

### äº¤äº’å¼æµ‹è¯•

```bash
python interactive_test.py --checkpoint outputs/qwen3-32b-grpo/final
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### æ¢å¤è®­ç»ƒ

```bash
python train_grpo.py --resume
```

### è‡ªå®šä¹‰è®­ç»ƒå‚æ•°

```bash
python train_grpo.py \
    --model unsloth/Qwen3-32B \
    --config configs/custom.yaml \
    --load_in_4bit
```

### ä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡

åœ¨é…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹ï¼š

```yaml
max_seq_length: 8192  # æˆ–æ›´å¤§
```

## ğŸ’¾ æ˜¾å­˜éœ€æ±‚

| é…ç½® | æ˜¾å­˜éœ€æ±‚ | æ¨èç¡¬ä»¶ |
|------|---------|---------|
| 4-bit + LoRA-16 | ~20GB | RTX 4090, A6000 |
| 4-bit + LoRA-32 | ~24GB | RTX 4090, A6000 |
| FP16 + LoRA-16 | ~40GB | A100-40GB |
| FP16 + LoRA-32 | ~48GB | A100-80GB |

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š
- å‡å° `per_device_train_batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- ä½¿ç”¨ `load_in_4bit: true`
- å‡å° `lora_r`

## ğŸ“ å…³äº Qwen3-32B

Qwen3-32B æ˜¯ Qwen3 ç³»åˆ—çš„æ——èˆ°æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- 32.8B å‚æ•°
- åŸç”Ÿæ”¯æŒ 32K ä¸Šä¸‹æ–‡ï¼Œå¯æ‰©å±•åˆ° 131K (ä½¿ç”¨ YaRN)
- æ”¯æŒæ€ç»´é“¾æ¨¡å¼åˆ‡æ¢
- ä¼˜ç§€çš„æ¨ç†ã€ä»£ç å’Œå¤šè¯­è¨€èƒ½åŠ›

è¯¦è§ï¼šhttps://huggingface.co/unsloth/Qwen3-32B

## ğŸ› æ•…éšœæ’æŸ¥

### CUDA å†…å­˜ä¸è¶³

```bash
# å‡å° batch size
python train_grpo.py --config configs/default.yaml
# ç„¶åä¿®æ”¹ configs/default.yaml:
# per_device_train_batch_size: 1
# gradient_accumulation_steps: 8
```

### Transformers ç‰ˆæœ¬é”™è¯¯

```bash
pip install transformers>=4.51.0
```

### Unsloth å®‰è£…é—®é¢˜

```bash
pip install "unsloth[cu124_ampere]>=2025.1" --upgrade
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨ MIT è®¸å¯è¯ã€‚

## ğŸ“š ä»»åŠ¡æ–‡æ¡£

- **æ•°å­¦æ¨ç†ï¼ˆGSM8Kï¼‰**ï¼šè§æœ¬ README
- **Link Search Agent**ï¼šè§ [LINKSEARCH_README.md](LINKSEARCH_README.md)

## ğŸ™ è‡´è°¢

- [TRL](https://github.com/huggingface/trl) - Transformer Reinforcement Learning
- [Unsloth](https://github.com/unslothai/unsloth) - é«˜æ•ˆè®­ç»ƒåŠ é€Ÿ
- [Qwen Team](https://github.com/QwenLM/Qwen) - Qwen3 æ¨¡å‹

## ğŸ“® è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issueã€‚
